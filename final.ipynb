{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzTz89gbSZTl"
      },
      "source": [
        "## Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zw9MdveQiKN2",
        "outputId": "210be1f7-eaab-4c7c-885a-be85a3aee615"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum fonts-ipafont-gothic\n",
            "  fonts-ipafont-mincho fonts-wqy-microhei fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 39.6 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 123623 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u422-b05-1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u422-b05-1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u422-b05-1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u422-b05-1~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u422-b05-1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u422-b05-1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Setup of the Spark\n",
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ueom28TxiE3P"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from collections import defaultdict\n",
        "\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql.functions import col, when\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import StructType, StructField, StringType, LongType, ArrayType, IntegerType\n",
        "\n",
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upmAXubMjKEp"
      },
      "outputs": [],
      "source": [
        "# Creating the session\n",
        "conf = SparkConf()\n",
        "conf.set(\"spark.ui.port\", \"4050\")\n",
        "conf.setAppName(\"DIS-project\")\n",
        "conf.setMaster(\"local[*]\")\n",
        "conf.set(\"spark.driver.memory\", \"8G\")\n",
        "conf.set(\"spark.driver.maxResultSize\", \"8g\")\n",
        "conf.set(\"spark.executor.memory\", \"4G\")\n",
        "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKjGCWx9-Vj5"
      },
      "source": [
        "## Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1not7wP3IwM"
      },
      "outputs": [],
      "source": [
        "def generate_synthetic_call_data(num_clients,\n",
        "                                 call_frequency_range,\n",
        "                                 call_duration_range,\n",
        "                                 time_range,\n",
        "                                 save_to_csv=False,\n",
        "                                 logs=False):\n",
        "    \"\"\"\n",
        "    Generates synthetic call data for a specified number of clients, with calls distributed across\n",
        "    multiple communities. The function returns the data as a PySpark DataFrame and optionally saves\n",
        "    it to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        num_clients (int): Total number of clients.\n",
        "        call_frequency_range (tuple): Min and max range of call frequency per client in each community.\n",
        "        call_duration_range (tuple): Min and max duration range for each call in minutes.\n",
        "        time_range (tuple): Start and end time range for calls, formatted as 'YYMMDDHHMM'.\n",
        "        save_to_csv (bool): If True, saves the resulting DataFrame to a CSV file. Default is False.\n",
        "        logs (bool): If True, logs intermediate steps and timing information. Default is False.\n",
        "\n",
        "    Returns:\n",
        "        pyspark.sql.DataFrame: A DataFrame containing generated call data with columns:\n",
        "            - 'c1': ID of the first client in the call.\n",
        "            - 'c2': ID of the second client in the call.\n",
        "            - 'start': Start time of the call (formatted as 'YYMMDDHHMM').\n",
        "            - 'end': End time of the call (formatted as 'YYMMDDHHMM').\n",
        "    \"\"\"\n",
        "\n",
        "    community_sizes, remaining_clients = [], num_clients\n",
        "    lower_size_proportion = 0.05\n",
        "    upper_size_proportion = 0.2\n",
        "\n",
        "    while remaining_clients / num_clients > 0.1:\n",
        "        # Split version 2\n",
        "        min_possible_size = int(remaining_clients * lower_size_proportion) if int(remaining_clients * lower_size_proportion) > 5 else 5\n",
        "        max_possible_size = int(remaining_clients * upper_size_proportion) if int(remaining_clients * upper_size_proportion) > min_possible_size else min_possible_size + 5\n",
        "        size = random.randint(min_possible_size, max_possible_size)\n",
        "        # print(f'Size = ({min_possible_size}, {max_possible_size}) = {size}')\n",
        "        community_sizes.append(size)\n",
        "        remaining_clients -= size\n",
        "\n",
        "    community_sizes.append(remaining_clients)  # The last community gets the remaining clients\n",
        "\n",
        "\n",
        "    # Step 1.3: Shuffle the clients list for randomness\n",
        "    # random.shuffle(clients)\n",
        "    shuffled_indices = list(range(1, num_clients + 1))\n",
        "    random.shuffle(shuffled_indices)\n",
        "\n",
        "    # Step 3: Assign clients to communities based on the predefined sizes\n",
        "    communities = []\n",
        "    current_index = 0\n",
        "\n",
        "    for size in community_sizes:\n",
        "        communities.append(shuffled_indices[current_index:current_index + size])\n",
        "        current_index += size\n",
        "    print(f\"Number of clients per community: {' '.join(map(str, [len(comm) for comm in communities]))}\")\n",
        "    print(f\"Total of {len(communities)} communities created.\")\n",
        "\n",
        "    call_data = []\n",
        "\n",
        "    # Step 5: Generate call data for each community\n",
        "    min_duration, max_duration = call_duration_range\n",
        "    start_time_str, end_time_str = time_range\n",
        "\n",
        "    start_time = datetime.strptime(start_time_str, '%y%m%d%H%M')\n",
        "    end_time = datetime.strptime(end_time_str, '%y%m%d%H%M')\n",
        "    time_diff_minutes = int((end_time - start_time).total_seconds() / 60)\n",
        "    s5 = time.time()\n",
        "    for community in communities:\n",
        "        # Introduce a community-level time shift to vary start times across communities\n",
        "        community_time_shift = random.randint(0, time_diff_minutes // 5)\n",
        "        community_start_time = start_time + timedelta(minutes=community_time_shift)\n",
        "\n",
        "        # Vary the duration range for each community to create different mean durations\n",
        "        community_min_duration = random.randint(min_duration, max_duration // 2)\n",
        "        community_max_duration = random.randint(max_duration // 2, max_duration)\n",
        "\n",
        "        # Vary call frequency for communities: some are more active, others less\n",
        "        community_call_frequency_range = (\n",
        "            random.randint(call_frequency_range[0] // 2, call_frequency_range[0]),\n",
        "            random.randint(call_frequency_range[1], call_frequency_range[1] * 2)\n",
        "        )\n",
        "\n",
        "        for client in community:\n",
        "            call_frequency = random.randint(community_call_frequency_range[0], community_call_frequency_range[1])\n",
        "            for _ in range(call_frequency):\n",
        "                other_client = random.choice(community)\n",
        "                while other_client == client:\n",
        "                    other_client = random.choice(community)\n",
        "                call_data.append(generate_call(client, other_client, community_start_time, time_diff_minutes, community_min_duration, community_max_duration))\n",
        "\n",
        "    e5 = time.time()\n",
        "    print(f\"Step 3 generate call data finished in {np.round(e5 - s5, 3)} seconds\") if logs else None\n",
        "\n",
        "    # Step 6: Create and return a PySpark DataFrame\n",
        "    s7 = time.time()\n",
        "    call_df = spark.createDataFrame(call_data, schema=['c1', 'c2', 'start', 'end'])\n",
        "    e7 = time.time()\n",
        "    print(f\"Step 6 create PySpark DataFrame finished in {np.round(e7 - s7, 3)} seconds\") if logs else None\n",
        "\n",
        "    tot_num_calls = call_df.count()\n",
        "    print(f\"Total number of calls: {tot_num_calls}\")\n",
        "    # Step 7: Optionally save to CSV\n",
        "    if save_to_csv:\n",
        "        s6 = time.time()\n",
        "        call_df.coalesce(1).write.csv(f'{tot_num_calls}_calls.csv', header=True, mode='overwrite')\n",
        "        e6 = time.time()\n",
        "        print(f\"Step 7 save to CSV finished in {np.round(e6 - s6, 3)} seconds\") if logs else None\n",
        "\n",
        "    return call_df\n",
        "\n",
        "\n",
        "def generate_call(client_a,\n",
        "                  client_b,\n",
        "                  start_time,\n",
        "                  time_diff_minutes,\n",
        "                  min_duration,\n",
        "                  max_duration):\n",
        "    \"\"\"\n",
        "    Generates a single call between two clients with a random start time within a given range\n",
        "    and a random duration within specified limits.\n",
        "\n",
        "    Args:\n",
        "        client_a (int): ID of the initiating client.\n",
        "        client_b (int): ID of the receiving client.\n",
        "        start_time (datetime.datetime): Start of the allowed time range for calls.\n",
        "        time_diff_minutes (int): Total allowed time range in minutes.\n",
        "        min_duration (int): Minimum duration for each call in minutes.\n",
        "        max_duration (int): Maximum duration for each call in minutes.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple representing the call details, with:\n",
        "            - client_a (int): ID of the initiating client.\n",
        "            - client_b (int): ID of the receiving client.\n",
        "            - call_start_str (str): Start time of the call (formatted as 'YYMMDDHHMM').\n",
        "            - call_end_str (str): End time of the call (formatted as 'YYMMDDHHMM').\n",
        "    \"\"\"\n",
        "    # Random call start time within the time range\n",
        "    random_minutes = random.randint(0, time_diff_minutes)\n",
        "    call_start_time = start_time + timedelta(minutes=random_minutes)\n",
        "\n",
        "    # Vary call duration by adding randomness to the duration range\n",
        "    call_duration = random.randint(min_duration, max_duration)\n",
        "    call_end_time = call_start_time + timedelta(minutes=call_duration)\n",
        "\n",
        "    # Randomly add variability to start and end times (like delays or extensions)\n",
        "    additional_random_minutes = random.randint(-5, 10)\n",
        "    call_start_time += timedelta(minutes=additional_random_minutes)\n",
        "    call_end_time += timedelta(minutes=random.randint(-2, 5))\n",
        "\n",
        "    call_start_str = call_start_time.strftime('%y%m%d%H%M')\n",
        "    call_end_str = call_end_time.strftime('%y%m%d%H%M')\n",
        "\n",
        "    return (client_a, client_b, call_start_str, call_end_str)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu87uXlS-X-T"
      },
      "source": [
        "## Community Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1H6nojuWoBcX"
      },
      "outputs": [],
      "source": [
        "def dfs(client, community, visited, graph):\n",
        "    \"\"\"\n",
        "    Performs a depth-first search (DFS) to find all connected nodes in a graph component starting from a given client.\n",
        "\n",
        "    Args:\n",
        "        client (int): The starting client node for DFS.\n",
        "        community (list): List to store clients in the current community.\n",
        "        visited (set): Set of visited clients to avoid re-processing.\n",
        "        graph (dict): Dictionary representing the graph with clients as keys and their connections as values.\n",
        "    \"\"\"\n",
        "    stack = [client]\n",
        "    visited.add(client)\n",
        "\n",
        "    while stack:\n",
        "        node = stack.pop()\n",
        "        community.append(node)\n",
        "        for neighbor in graph[node]:\n",
        "            if neighbor not in visited:\n",
        "                visited.add(neighbor)\n",
        "                stack.append(neighbor)\n",
        "\n",
        "def find_communities(call_df, logs=False):\n",
        "    \"\"\"\n",
        "    Identifies communities of clients from call data using DFS to find connected components.\n",
        "\n",
        "    Args:\n",
        "        call_df (pyspark.sql.DataFrame): DataFrame containing call data with columns:\n",
        "            - 'c1': ID of the first client in a call.\n",
        "            - 'c2': ID of the second client in a call.\n",
        "        logs (bool): If True, logs the execution time of each step. Default is False.\n",
        "\n",
        "    Returns:\n",
        "        pyspark.RDD: An RDD of (client, community_number) pairs, where each client is assigned a community.\n",
        "\n",
        "    Steps:\n",
        "        1. Build a graph of unique client connections from the call data.\n",
        "        2. Aggregate data to create a list of connections for each client.\n",
        "        3. Collect the graph as a dictionary to enable DFS traversal.\n",
        "        4. Perform DFS to identify connected components, each representing a community.\n",
        "        5. Convert the client-community mapping to an RDD for efficient processing.\n",
        "        6. Validate the results by counting unique communities.\n",
        "    \"\"\"\n",
        "    # Step 1: Build the graph from call data in a distributed manner\n",
        "    s1 = time.time()\n",
        "    graph_df = call_df.select(\"c1\", \"c2\").distinct()\n",
        "    e1 = time.time()\n",
        "    print(f\"Step 1 build graph finished in {np.round(e1 - s1, 3)} seconds\") if logs else None\n",
        "\n",
        "    # Step 2: Aggregate the data to create connections between clients\n",
        "    s2 = time.time()\n",
        "    client_graph = (\n",
        "        graph_df\n",
        "        .withColumn(\"client_pair\", F.array(\"c1\", \"c2\"))\n",
        "        .select(F.explode(\"client_pair\").alias(\"client\"), \"c1\", \"c2\")\n",
        "        .groupBy(\"client\")\n",
        "        .agg(F.collect_set(F.when(F.col(\"client\") == F.col(\"c1\"), F.col(\"c2\")).otherwise(F.col(\"c1\"))).alias(\"connections\"))\n",
        "    )\n",
        "    # print(client_graph.collect())\n",
        "    # print(client_graph.count())\n",
        "    e2 = time.time()\n",
        "    print(f\"Step 2 aggregate data finished in {np.round(e2 - s2, 3)} seconds\") if logs else None\n",
        "    # print(f\"Client graph: {client_graph.collect()}\")\n",
        "\n",
        "    # Step 3: Collect the graph as a dictionary for DFS traversal\n",
        "    s3 = time.time()\n",
        "    graph = defaultdict(list)\n",
        "    for row in client_graph.collect():\n",
        "        client = row[\"client\"]\n",
        "        connections = row[\"connections\"]\n",
        "        graph[client].extend(connections)\n",
        "    e3 = time.time()\n",
        "    print(f\"Step 3 collect graph finished in {np.round(e3 - s3, 3)} seconds\") if logs else None\n",
        "\n",
        "    # V2\n",
        "    s4 = time.time()\n",
        "    visited = set()\n",
        "    client_to_community = {}\n",
        "    community_counter = 1\n",
        "    community_sizes = {}\n",
        "\n",
        "    for client in graph:\n",
        "        if client not in visited:\n",
        "            community = []\n",
        "            dfs(client, community, visited, graph)\n",
        "\n",
        "            # Only add the community if it has at least 5 clients\n",
        "            if len(community) >= 5:\n",
        "                for member in community:\n",
        "                    client_to_community[member] = community_counter\n",
        "                community_sizes[community_counter] = len(community)\n",
        "                community_counter += 1\n",
        "    e4 = time.time()\n",
        "    print(f\"Step 4 DFS traversal finished in {np.round(e4 - s4, 3)} seconds\") if logs else None\n",
        "    # print(f\"Client to community: {client_to_community}\")\n",
        "    # Step 5: Return RDD of (client, community_number)\n",
        "    s5 = time.time()\n",
        "    client_to_community_rdd = spark.sparkContext.parallelize([(client, comm_number) for client, comm_number in client_to_community.items()])\n",
        "    e5 = time.time()\n",
        "    print(f\"Step 5 convert to RDD finished in {np.round(e5 - s5, 3)} seconds\") if logs else None\n",
        "\n",
        "    # Step 6: Validate result by counting unique communities\n",
        "    s6 = time.time()\n",
        "    unique_comms_count = client_to_community_rdd.map(lambda x: x[1]).distinct().count()\n",
        "    e6 = time.time()\n",
        "    print(f\"Step 6 count unique communities finished in {np.round(e6 - s6, 3)} seconds\") if logs else None\n",
        "    print(f\"Number of unique communities found: {unique_comms_count}\")\n",
        "\n",
        "    return client_to_community_rdd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4FoMIbJpvYf"
      },
      "source": [
        "## Similarity Check - Ready"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeatnyvKWX7n"
      },
      "outputs": [],
      "source": [
        "def normalized_difference(val1, val2):\n",
        "    \"\"\"\n",
        "    Calculate the normalized difference between two values, accounting for None values.\n",
        "\n",
        "    Parameters:\n",
        "        val1 (float or None): First value to compare.\n",
        "        val2 (float or None): Second value to compare.\n",
        "\n",
        "    Returns:\n",
        "        float: Normalized difference between val1 and val2, or 0 if either value is None\n",
        "               or their maximum is zero.\n",
        "    \"\"\"\n",
        "    if val1 is None or val2 is None:\n",
        "        return 0  # Or another appropriate default value\n",
        "\n",
        "    # Normalize difference based on the maximum of the two values\n",
        "    norm_diff = np.abs(val1 - val2) / np.max([val1, val2]) if np.max([val1, val2]) != 0 else 0\n",
        "    return norm_diff\n",
        "\n",
        "def compute_similarity(comm1_number: int, comm2_number: int,\n",
        "                       num_clients1: int, mean_duration1: float, stddev_duration1: float, total_duration1: float,\n",
        "                       num_clients2: int, mean_duration2: float, stddev_duration2: float, total_duration2: float,\n",
        "                       logs=False) -> float:\n",
        "    \"\"\"\n",
        "    Compute the similarity score between two communities based on normalized differences\n",
        "    in metrics like number of clients, mean call duration, standard deviation, and total duration.\n",
        "\n",
        "    Parameters:\n",
        "        comm1_number (int): Community number for the first community.\n",
        "        comm2_number (int): Community number for the second community.\n",
        "        num_clients1 (int): Number of clients in the first community.\n",
        "        mean_duration1 (float): Mean call duration in the first community.\n",
        "        stddev_duration1 (float): Standard deviation of call duration in the first community.\n",
        "        total_duration1 (float): Total call duration in the first community.\n",
        "        num_clients2 (int): Number of clients in the second community.\n",
        "        mean_duration2 (float): Mean call duration in the second community.\n",
        "        stddev_duration2 (float): Standard deviation of call duration in the second community.\n",
        "        total_duration2 (float): Total call duration in the second community.\n",
        "        logs (bool): If True, print detailed logs for each similarity component.\n",
        "\n",
        "    Returns:\n",
        "        float: Similarity score between the two communities (1 indicates identical communities).\n",
        "    \"\"\"\n",
        "\n",
        "    secondary_prints = 0\n",
        "    print(f\"{comm1_number} <=> {comm2_number}.\", end=\" \") if logs else None\n",
        "\n",
        "    norm_num_clients = normalized_difference(num_clients1, num_clients2)\n",
        "    print(f\"Normalized number of clients difference: {norm_num_clients:.4f}\") if secondary_prints else None\n",
        "\n",
        "    norm_mean_duration = normalized_difference(mean_duration1, mean_duration2)\n",
        "    print(f\"Normalized mean duration difference: {norm_mean_duration:.4f}\") if secondary_prints else None\n",
        "\n",
        "    norm_stddev_duration = normalized_difference(stddev_duration1, stddev_duration2)\n",
        "    print(f\"Normalized stddev duration difference: {norm_stddev_duration:.4f}\") if secondary_prints else None\n",
        "\n",
        "    norm_total_duration = normalized_difference(total_duration1, total_duration2)\n",
        "    print(f\"Normalized total duration difference: {norm_total_duration:.4f}\") if secondary_prints else None\n",
        "\n",
        "    # Compute similarity score (as per previous formulas)\n",
        "    similarity_score = 1 - (1 / 4) * (norm_num_clients + norm_mean_duration + norm_stddev_duration + norm_total_duration)\n",
        "\n",
        "    # Print the final similarity score\n",
        "    print(f\"= {similarity_score:.4f}\") if logs else None\n",
        "\n",
        "    return similarity_score\n",
        "\n",
        "\n",
        "def group_similar_communities(call_df: DataFrame,\n",
        "                              client_to_community_rdd,\n",
        "                              filename,\n",
        "                              save_to_txt=False,\n",
        "                              logs=True) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Group similar communities based on similarity metrics calculated from call data.\n",
        "\n",
        "    Parameters:\n",
        "        call_df (DataFrame): DataFrame containing call data, with columns for client IDs and call times.\n",
        "        client_to_community_rdd (RDD): RDD mapping clients to their respective communities.\n",
        "        filename (str): Filename to save community groupings if `save_to_txt` is True.\n",
        "        save_to_txt (bool): If True, save grouped communities to a text file.\n",
        "        logs (bool): If True, print detailed logs for each processing step.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: A DataFrame with each row representing a group of similar communities.\n",
        "    \"\"\"\n",
        "    THRESHOLD = 0.8  # Set threshold for similarity score\n",
        "\n",
        "    # Step 1: Convert RDD to DataFrame and join with the original call data\n",
        "    s1 = time.time()\n",
        "\n",
        "    comm_df = client_to_community_rdd.toDF([\"client\", \"community_number\"])\n",
        "\n",
        "    enriched_df = call_df.join(comm_df, call_df[\"c1\"] == comm_df[\"client\"], \"left\").drop(\"client\")\n",
        "    enriched_df = enriched_df.withColumnRenamed(\"community_number\", \"comm_number_c1\")\n",
        "    enriched_df = enriched_df.join(comm_df, call_df[\"c2\"] == comm_df[\"client\"], \"left\").drop(\"client\")\n",
        "    enriched_df = enriched_df.withColumnRenamed(\"community_number\", \"comm_number_c2\")\n",
        "\n",
        "    enriched_df = enriched_df.withColumn(\n",
        "        \"comm_number\", F.coalesce(F.col(\"comm_number_c1\"), F.col(\"comm_number_c2\"))\n",
        "    ).drop(\"comm_number_c1\", \"comm_number_c2\").dropna()\n",
        "\n",
        "\n",
        "\n",
        "    # print(enriched_df.select('comm_number').distinct().show(n=25, truncate = False))\n",
        "    # print(f\"Number of unique communities: {enriched_df.select('comm_number').distinct().count()}\")\n",
        "\n",
        "    # Convert durations to minutes (assuming 'start' and 'end' are in seconds)\n",
        "    enriched_df = enriched_df.withColumn(\"duration\", (enriched_df[\"end\"] - enriched_df[\"start\"]) / 60.0)\n",
        "    e1 = time.time()\n",
        "    print(f\"Step 1 convert RDD to DataFrame and join with the original call data finished {np.round(e1 - s1, 3)} seconds\") if logs else None\n",
        "\n",
        "    # Step 2: Calculate community metrics\n",
        "    s2 = time.time()\n",
        "    community_metrics = (\n",
        "        enriched_df.groupBy(\"comm_number\")\n",
        "        .agg(\n",
        "            F.countDistinct(\"c1\").alias(\"num_clients\"),\n",
        "            F.mean(\"duration\").alias(\"mean_duration\"),\n",
        "            F.stddev(\"duration\").alias(\"stddev_duration\"),\n",
        "            F.sum(\"duration\").alias(\"total_duration\")\n",
        "        )\n",
        "    )\n",
        "    e2 = time.time()\n",
        "    print(f\"Step 2 calculate community metrics finished {np.round(e2 - s2, 3)} seconds\") if logs else None\n",
        "\n",
        "    # Step 3: Compare communities using similarity formula\n",
        "    s3 = time.time()\n",
        "    community_data = community_metrics.collect()\n",
        "\n",
        "    similar_communities = []\n",
        "    assigned_groups = {}\n",
        "\n",
        "    for i in range(len(community_data)):\n",
        "        for j in range(i + 1, len(community_data)):\n",
        "            comm1 = community_data[i]\n",
        "            comm2 = community_data[j]\n",
        "\n",
        "            # Calculate similarity using the combined similarity score\n",
        "            similarity_value = compute_similarity(\n",
        "                  comm1.comm_number, comm2.comm_number,\n",
        "                  comm1.num_clients, comm1.mean_duration, comm1.stddev_duration, comm1.total_duration,\n",
        "                  comm2.num_clients, comm2.mean_duration, comm2.stddev_duration, comm2.total_duration\n",
        "              )\n",
        "\n",
        "            # Store similarity results\n",
        "            similar_communities.append((comm1.comm_number, comm2.comm_number, similarity_value))\n",
        "\n",
        "            if similarity_value >= THRESHOLD:\n",
        "                # Ensure communities only belong to one group\n",
        "                group1 = assigned_groups.get(comm1.comm_number, set())\n",
        "                group2 = assigned_groups.get(comm2.comm_number, set())\n",
        "\n",
        "                if not group1 and not group2:\n",
        "                    # Create a new group for both communities\n",
        "                    new_group = {comm1.comm_number, comm2.comm_number}\n",
        "                    assigned_groups[comm1.comm_number] = new_group\n",
        "                    assigned_groups[comm2.comm_number] = new_group\n",
        "                elif group1 and not group2:\n",
        "                    # Add comm2 to comm1's group\n",
        "                    group1.add(comm2.comm_number)\n",
        "                    assigned_groups[comm2.comm_number] = group1\n",
        "                elif group2 and not group1:\n",
        "                    # Add comm1 to comm2's group\n",
        "                    group2.add(comm1.comm_number)\n",
        "                    assigned_groups[comm1.comm_number] = group2\n",
        "                else:\n",
        "                    # Merge the two groups\n",
        "                    merged_group = group1.union(group2)\n",
        "                    for comm in merged_group:\n",
        "                        assigned_groups[comm] = merged_group\n",
        "    e3 = time.time()\n",
        "    print(f\"Step 3 compare communities using similarity formula finished {np.round(e3 - s3, 3)} seconds\") if logs else None\n",
        "\n",
        "\n",
        "    # Step 4: Group similar communities and save final groups to a text file\n",
        "    s4 = time.time()\n",
        "    final_groups = {}\n",
        "    for comm, group in assigned_groups.items():\n",
        "        group_key = tuple(sorted(group))  # Use sorted tuple as a key for grouping\n",
        "        if group_key not in final_groups:\n",
        "            final_groups[group_key] = group_key\n",
        "\n",
        "    # Ensure that ungrouped communities are also included in the final output\n",
        "    ungrouped_communities = set(comm.comm_number for comm in community_data) - set(assigned_groups.keys())\n",
        "    if ungrouped_communities:  # Check if there are any ungrouped communities\n",
        "        for comm in ungrouped_communities:\n",
        "            final_groups[frozenset({comm})] = frozenset({comm})\n",
        "    e4 = time.time()\n",
        "    print(f\"Step 4 group similar communities took {np.round(e4 - s4, 3)} seconds\") if logs else None\n",
        "\n",
        "    # Step 5 printing and saving\n",
        "    gr_num = 1\n",
        "    s5 = time.time()\n",
        "    if save_to_txt:\n",
        "        # Step 5.1: Group the DataFrame by communities and collect all the necessary data at once\n",
        "        s51 = time.time()\n",
        "        community_calls = (\n",
        "            enriched_df.groupBy(\"comm_number\")\n",
        "            .agg(F.collect_list(F.struct(\"c1\", \"c2\", \"start\", \"end\")).alias(\"calls\"))\n",
        "            .collect()\n",
        "        )\n",
        "        e51 = time.time()\n",
        "        print(f\"Step 5.1 collect all data at once finished {np.round(e51 - s51, 3)} seconds\") if logs else None\n",
        "\n",
        "        # Step 5.2: Build the text output in memory and then write it all at once\n",
        "        s52 = time.time()\n",
        "        output_lines = []\n",
        "\n",
        "        for group_key in final_groups:\n",
        "            output_lines.append(f\"Group {gr_num}:\\n\")\n",
        "            print(f\"Group {gr_num}: \" + \", \".join(f\"c{comm}\" for comm in group_key))\n",
        "            comm_num = 1\n",
        "\n",
        "            for comm in group_key:\n",
        "                output_lines.append(f\"  Community {comm_num}:\\n\")\n",
        "\n",
        "                # Find the community data in the pre-collected results\n",
        "                comm_data = next((comm_row.calls for comm_row in community_calls if comm_row.comm_number == comm), [])\n",
        "\n",
        "                # Format the call data for each community\n",
        "                for call in comm_data:\n",
        "                    output_lines.append(f\"    {call['c1']}, {call['c2']}, {call['start']}, {call['end']}\\n\")\n",
        "\n",
        "                comm_num += 1\n",
        "\n",
        "            output_lines.append(\"\\n\")\n",
        "            gr_num += 1\n",
        "        e52 = time.time()\n",
        "        print(f\"Step 5.2 build text output in memory finished {np.round(e52 - s52, 3)} seconds\") if logs else None\n",
        "\n",
        "        # Step 5.3: Write all lines to the file at once\n",
        "        s53 = time.time()\n",
        "        with open(f'{filename}.txt', 'w') as f:\n",
        "            f.writelines(output_lines)\n",
        "        e53 = time.time()\n",
        "        print(f\"Step 5.3 write all lines to the file {np.round(e53 - s53, 3)} seconds\") if logs else None\n",
        "    else:\n",
        "        for group_key in final_groups:\n",
        "            print(f\"Group {gr_num}: \" + \", \".join(f\"c{comm}\" for comm in group_key))\n",
        "            gr_num += 1\n",
        "    e5 = time.time()\n",
        "    print(f\"Step 5 write all lines to the file at once finished {np.round(e5 - s5, 3)} seconds\") if logs else None\n",
        "\n",
        "    # Prepare DataFrame output (if needed for further processing)\n",
        "    group_data = [(list(group),) for group in final_groups.values()]\n",
        "    schema = StructType([StructField(\"grouped_communities\", ArrayType(IntegerType()), True)])\n",
        "    return spark.createDataFrame(group_data, schema=schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pKmtzz02eQ9"
      },
      "source": [
        "## Simple test run one call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCEWJuzcjuoF",
        "outputId": "7a2ee72d-f633-4754-e95e-d91bee5cc768"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of clients per community: 1444 1257 838 449 415 1002 275 678 521 353 447 349 244 132 170 284 174 968\n",
            "Total of 18 communities created.\n",
            "Total number of calls: 24032\n",
            "Execution time for data creation: 13.149 seconds\n",
            "__________________________________________________\n",
            "Step 1 build graph finished in 0.128 seconds\n",
            "Step 2 aggregate data finished in 0.263 seconds\n",
            "Step 3 collect graph finished in 4.349 seconds\n",
            "Step 4 DFS traversal finished in 0.018 seconds\n",
            "Step 5 convert to RDD finished in 0.015 seconds\n",
            "Step 6 count unique communities finished in 1.3 seconds\n",
            "Number of unique communities found: 18\n",
            "Execution time for community finding: 6.079 seconds\n",
            "Number of clients here  9835\n",
            "__________________________________________________\n",
            "Step 1 convert RDD to DataFrame and join with the original call data finished 0.556 seconds\n",
            "Step 2 calculate community metrics finished 0.097 seconds\n",
            "Step 3 compare communities using similarity formula finished 2.989 seconds\n",
            "Step 4 group similar communities took 0.0 seconds\n",
            "Group 1: c2, c7, c8, c11\n",
            "Group 2: c1, c5, c6, c12, c13, c14, c16\n",
            "Group 3: c3, c4, c10, c15, c17\n",
            "Group 4: c9\n",
            "Group 5: c18\n",
            "Step 5 write all lines to the file at once finished 0.002 seconds\n",
            "Execution time for community grouping: 3.68 seconds\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Generate synthetic call data\n",
        "start_time_data_creation = time.time()\n",
        "\n",
        "call_df = generate_synthetic_call_data(\n",
        "    num_clients=10000,\n",
        "    call_frequency_range=(1, 3),\n",
        "    call_duration_range=(20, 600000),\n",
        "    time_range=('2201010000', '2412312359'),\n",
        "    save_to_csv=False,\n",
        "    logs=False\n",
        ")\n",
        "\n",
        "end_time_data_creation = time.time()\n",
        "time_secs_data_creation = np.round(end_time_data_creation - start_time_data_creation, 3)\n",
        "print(f\"Execution time for data creation: {time_secs_data_creation} seconds\")\n",
        "print('_____'*10)\n",
        "\n",
        "# ______________________________________________________________________________________________________\n",
        "# Step 2: Find communities and assign community numbers (now returns DataFrame with `comm_number`)\n",
        "start_time_community_finding = time.time()\n",
        "\n",
        "comms_rdd = find_communities(call_df, logs=True)\n",
        "\n",
        "end_time_community_finding = time.time()\n",
        "time_secs_community_finding = np.round(end_time_community_finding - start_time_community_finding, 3)\n",
        "print(f\"Execution time for community finding: {time_secs_community_finding} seconds\")\n",
        "\n",
        "comms_list = comms_rdd.collect()\n",
        "community_counts = {}\n",
        "\n",
        "for client, community in comms_list:\n",
        "    community_counts[community] = community_counts.get(community, 0) + 1  # Use get with default value of 0\n",
        "\n",
        "all_count = 0\n",
        "for community, count in community_counts.items():\n",
        "    # print(f\"Community {community} has {count} clients.\")\n",
        "    all_count += count\n",
        "\n",
        "print('Number of clients here ', all_count)\n",
        "print('_____'*10)\n",
        "\n",
        "\n",
        "# ______________________________________________________________________________________________________\n",
        "# Step 3: Group similar communities (this step will still work with the community DataFrame)\n",
        "start_time_similarity_check = time.time()\n",
        "\n",
        "groups = group_similar_communities(call_df, comms_rdd, 'a', save_to_txt=False, logs=True)\n",
        "\n",
        "end_time_similarity_check = time.time()\n",
        "time_secs_similarity_check = np.round(end_time_similarity_check - start_time_similarity_check, 3)\n",
        "print(f\"Execution time for community grouping: {time_secs_similarity_check} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2dmm3QiPmx8"
      },
      "source": [
        "Toy Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBH4B_HYcy0r",
        "outputId": "28125ee5-16dc-4d86-d561-439f2fa74172"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+---+----------+----------+\n",
            "| c1| c2|     start|       end|\n",
            "+---+---+----------+----------+\n",
            "|  1|  2|2408010800|2408010830|\n",
            "|  2|  3|2408010900|2408010930|\n",
            "|  1|  3|2408011000|2408011030|\n",
            "|  4|  5|2408011100|2408011130|\n",
            "|  5|  6|2408011200|2408011230|\n",
            "|  6|  7|2408011300|2408011330|\n",
            "|  4|  7|2408011400|2408011430|\n",
            "|  8|  9|2408011500|2408011530|\n",
            "+---+---+----------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# test1.csv\n",
        "data = [\n",
        "    (1, 4, '2408010800', '2408010830'),\n",
        "    (4, 6, '2408021000', '2408021030'),\n",
        "    (1, 6, '2408021100', '2408021130'),\n",
        "    (2, 3, '2408010900', '2408010930'),\n",
        "    (3, 5, '2408010930', '2408011000'),\n",
        "    (5, 7, '2408021400', '2408021430'),\n",
        "    (2, 7, '2408030800', '2408030830'),\n",
        "]\n",
        "\n",
        "# test2.csv\n",
        "data = [\n",
        "    (1, 2, '2408010800', '2408010830'),\n",
        "    (2, 3, '2408010900', '2408010930'),\n",
        "    (1, 3, '2408011000', '2408011030'),\n",
        "    (4, 5, '2408011100', '2408011130'),\n",
        "    (5, 6, '2408011200', '2408011230'),\n",
        "    (6, 7, '2408011300', '2408011330'),\n",
        "    (4, 7, '2408011400', '2408011430'),\n",
        "    (8, 9, '2408011500', '2408011530')\n",
        "]\n",
        "\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"c1\", LongType(), True),\n",
        "    StructField(\"c2\", LongType(), True),\n",
        "    StructField(\"start\", StringType(), True),\n",
        "    StructField(\"end\", StringType(), True)\n",
        "])\n",
        "\n",
        "call_df = spark.createDataFrame(data, schema=schema)\n",
        "call_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSByMWut-gBO"
      },
      "source": [
        "## Test Runs All Together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMocD1lfiiQe"
      },
      "outputs": [],
      "source": [
        "clients_call_freq_pairs = [\n",
        "    [10_000, (1, 3)],\n",
        "    [10_000, (4, 6)],\n",
        "    [100_000, (1, 3)],\n",
        "    [100_000, (4, 6)],\n",
        "    [500_000, (1, 3)],\n",
        "    [500_000, (4, 6)],\n",
        "    [1_000_000, (1, 3)],\n",
        "    [1_000_000, (4, 6)],\n",
        "    [2_000_000, (1, 3)],\n",
        "]\n",
        "\n",
        "results = {\n",
        "    'num_clients': [],\n",
        "    'num_comms': [],\n",
        "    'num_calls': [],\n",
        "    'call_freq': [],\n",
        "    'time_secs_data_creation': [],\n",
        "    'time_secs_community_finding': [],\n",
        "    'time_secs_similarity_finding': [],\n",
        "    'time_secs_total': [],\n",
        "}\n",
        "\n",
        "for params in clients_call_freq_pairs:\n",
        "    num_client = params[0]\n",
        "    call_frequency = params[1]\n",
        "\n",
        "    print(f\"For {num_client} clients...\")\n",
        "    # Step 1: Generate synthetic call data\n",
        "    start_time_data_creation = time.time()\n",
        "\n",
        "    call_df = generate_synthetic_call_data(\n",
        "        num_clients=num_client,\n",
        "        call_frequency_range=call_frequency,\n",
        "        call_duration_range=(20, 600000),\n",
        "        time_range=('2201010000', '2412312359'),\n",
        "        save_to_csv=True,\n",
        "        logs=True\n",
        "    )\n",
        "\n",
        "    # Print length of the created df\n",
        "    ncalls = call_df.count()\n",
        "    print(f\"Total number of calls will be processed: {ncalls}\")\n",
        "    end_time_data_creation = time.time()\n",
        "    time_secs_data_creation = np.round(end_time_data_creation - start_time_data_creation, 3)\n",
        "    print(f\"Execution time for data creation: {time_secs_data_creation} seconds\")\n",
        "    print(\"_________\"*7)\n",
        "\n",
        "    # ______________________________________________________________________________________________________\n",
        "    # Step 2: Find communities and assign community numbers (now returns DataFrame with `comm_number`)\n",
        "    start_time_community_finding = time.time()\n",
        "\n",
        "    comms_rdd = find_communities(call_df, logs=False)\n",
        "\n",
        "    end_time_community_finding = time.time()\n",
        "    time_secs_community_finding = np.round(end_time_community_finding - start_time_community_finding, 3)\n",
        "    print(f\"Execution time for community finding: {time_secs_community_finding} seconds\")\n",
        "\n",
        "    num_comms = comms_rdd.map(lambda x: x[1]).distinct().count()\n",
        "\n",
        "    # ______________________________________________________________________________________________________\n",
        "    # Step 3: Group similar communities (this step will still work with the community DataFrame)\n",
        "    start_time_similarity_check = time.time()\n",
        "\n",
        "    groups = group_similar_communities(call_df, comms_rdd, num_client, logs=False, save_to_txt=True)\n",
        "\n",
        "    end_time_similarity_check = time.time()\n",
        "    time_secs_similarity_check = np.round(end_time_similarity_check - start_time_similarity_check, 3)\n",
        "    print(f\"Execution time for community grouping: {time_secs_similarity_check} seconds\")\n",
        "    print(\"_________\"*7)\n",
        "\n",
        "    # ______________________________________________________________________________________________________\n",
        "    # Calculate total time and store results\n",
        "    total_time = np.round(\n",
        "        time_secs_data_creation + time_secs_community_finding + time_secs_similarity_check, 3\n",
        "    )\n",
        "    print(f\"Total execution time: {total_time} seconds\")\n",
        "\n",
        "    results['num_clients'].append(num_client)\n",
        "    results['num_calls'].append(ncalls)\n",
        "    results['call_freq'].append(call_frequency)  # Add call frequency as a fixed value\n",
        "    results['time_secs_data_creation'].append(time_secs_data_creation)\n",
        "    results['time_secs_community_finding'].append(time_secs_community_finding)\n",
        "    results['time_secs_similarity_finding'].append(time_secs_similarity_check)\n",
        "    results['time_secs_total'].append(total_time)\n",
        "    results['num_comms'].append(num_comms)\n",
        "    print(\"=========\"*7)\n",
        "\n",
        "# Convert results to DataFrame and save to CSV\n",
        "result_df = pd.DataFrame(results)\n",
        "result_df.to_csv('performance_results.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzn65Lb0LiJk"
      },
      "source": [
        "## Tests with bigger sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJpjbXd1crOm"
      },
      "outputs": [],
      "source": [
        "clients_call_freq_pairs = [\n",
        "    [2_500_000, (1, 3)],\n",
        "    [2_500_000, (4, 6)],\n",
        "    [4_000_000, (1, 3)],\n",
        "    [4_000_000, (4, 6)],\n",
        "    [6_000_000, (1, 3)],\n",
        "    [6_000_000, (4, 6)],\n",
        "]\n",
        "\n",
        "results = {\n",
        "    'num_clients': [],\n",
        "    'num_comms': [],\n",
        "    'num_calls': [],\n",
        "    'call_freq': [],\n",
        "    'time_secs_data_creation': [],\n",
        "    'time_secs_community_finding': [],\n",
        "    'time_secs_similarity_finding': [],\n",
        "    'time_secs_total': [],\n",
        "}\n",
        "\n",
        "for params in clients_call_freq_pairs:\n",
        "    num_client = params[0]\n",
        "    call_frequency = params[1]\n",
        "\n",
        "    print(f\"For {num_client} clients...\")\n",
        "    # Step 1: Generate synthetic call data\n",
        "    start_time_data_creation = time.time()\n",
        "\n",
        "    call_df = generate_synthetic_call_data(\n",
        "        num_clients=num_client,\n",
        "        call_frequency_range=call_frequency,\n",
        "        call_duration_range=(20, 600000),\n",
        "        time_range=('2201010000', '2412312359'),\n",
        "        save_to_csv=True,\n",
        "        logs=True\n",
        "    )\n",
        "\n",
        "    # Print length of the created df\n",
        "    ncalls = call_df.count()\n",
        "    print(f\"Total number of calls will be processed: {ncalls}\")\n",
        "    end_time_data_creation = time.time()\n",
        "    time_secs_data_creation = np.round(end_time_data_creation - start_time_data_creation, 3)\n",
        "    print(f\"Execution time for data creation: {time_secs_data_creation} seconds\")\n",
        "    print(\"_________\"*7)\n",
        "\n",
        "    # ______________________________________________________________________________________________________\n",
        "    # Step 2: Find communities and assign community numbers (now returns DataFrame with `comm_number`)\n",
        "    start_time_community_finding = time.time()\n",
        "\n",
        "    comms_rdd = find_communities(call_df, logs=False)\n",
        "\n",
        "    end_time_community_finding = time.time()\n",
        "    time_secs_community_finding = np.round(end_time_community_finding - start_time_community_finding, 3)\n",
        "    print(f\"Execution time for community finding: {time_secs_community_finding} seconds\")\n",
        "\n",
        "    num_comms = comms_rdd.map(lambda x: x[1]).distinct().count()\n",
        "\n",
        "    # ______________________________________________________________________________________________________\n",
        "    # Step 3: Group similar communities (this step will still work with the community DataFrame)\n",
        "    start_time_similarity_check = time.time()\n",
        "\n",
        "    groups = group_similar_communities(call_df, comms_rdd, num_client, logs=False, save_to_txt=True)\n",
        "\n",
        "    end_time_similarity_check = time.time()\n",
        "    time_secs_similarity_check = np.round(end_time_similarity_check - start_time_similarity_check, 3)\n",
        "    print(f\"Execution time for community grouping: {time_secs_similarity_check} seconds\")\n",
        "    print(\"_________\"*7)\n",
        "\n",
        "    # ______________________________________________________________________________________________________\n",
        "    # Calculate total time and store results\n",
        "    total_time = np.round(\n",
        "        time_secs_data_creation + time_secs_community_finding + time_secs_similarity_check, 3\n",
        "    )\n",
        "    print(f\"Total execution time: {total_time} seconds\")\n",
        "\n",
        "    results['num_clients'].append(num_client)\n",
        "    results['num_calls'].append(ncalls)\n",
        "    results['call_freq'].append(call_frequency)  # Add call frequency as a fixed value\n",
        "    results['time_secs_data_creation'].append(time_secs_data_creation)\n",
        "    results['time_secs_community_finding'].append(time_secs_community_finding)\n",
        "    results['time_secs_similarity_finding'].append(time_secs_similarity_check)\n",
        "    results['time_secs_total'].append(total_time)\n",
        "    results['num_comms'].append(num_comms)\n",
        "    print(\"=========\"*7)\n",
        "\n",
        "# Convert results to DataFrame and save to CSV\n",
        "result_df_2 = pd.DataFrame(results)\n",
        "result_df_2.to_csv('performance_results_2.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NsMx7KHDWlb1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "gzTz89gbSZTl",
        "EKjGCWx9-Vj5",
        "lu87uXlS-X-T",
        "n4FoMIbJpvYf",
        "7pKmtzz02eQ9",
        "nM2mfOW_MM4M"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}